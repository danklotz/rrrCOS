---
title: "Code for Exploring Objective Functions"
author: "Daniel Klotz"
date: "12 Juli 2016"
output:
  html_document:
    number_sections: yes
  pdf_document:
    number_sections: yes
---


```{r setup, include=FALSE, purl=FALSE}
  knitr::opts_chunk$set(eval = FALSE, tidy = FALSE)
```

```{r, eval = TRUE, echo = FALSE, message=FALSE, purl=FALSE}
  require("ggplot2")
  require("hydroGOF")
```

# Introduction
This chapter defines the function necessary to calculate the objective functions
and visualise or explore them in different ways.
Currently 4 objective functions are used in **visCOS**. They are calculated from
the data columns of a data.frame defined by `viscos_options$name_data1` and
`viscos_options$name_data2`.Assuming that `name_data1` is $x$ and `name_data1`
is $y$ they are:

```{r, eval = TRUE, echo=FALSE, purl=FALSE}
  # here we define some data for the demonstrational plots
  number_of_data <- 200
  mean <- 0
  sd <- 1
  x <- rnorm(number_of_data,mean,sd)
  noise <- rnorm(number_of_data,mean,sd)
  demo_data <- data.frame(
    x = x,
    # y:
    y_0 = noise,
    y_m1 = -x,
    y_1 = x,
    y_2 = x^2,
    y_mh = -x + noise,
    y_h = x + noise
  )  
  demo_names <- names(demo_data)
  demo_length <- length(demo_names)

  # since I do not want to many dependencies and need to plot more functions into
  # one plot I am gonna use the `multiplot` function by Winson Chang
# see: http://www.cookbook-r.com/Graphs/Multiple_graphs_on_one_page_(ggplot2)/
multiplot <- function(..., plotlist=NULL, cols) {
    require(grid, quietly = TRUE)

    # Make a list from the ... arguments and plotlist
    plots <- c(list(...), plotlist)

    numPlots = length(plots)

    # Make the panel
    plotCols = cols                          # Number of columns of plots
    plotRows = ceiling(numPlots/plotCols) # Number of rows needed, calculated from # of cols

    # Set up the page
    grid.newpage()
    pushViewport(viewport(layout = grid.layout(plotRows, plotCols)))
    vplayout <- function(x, y)
        viewport(layout.pos.row = x, layout.pos.col = y)

    # Make each plot, in the correct location
    for (i in 1:numPlots) {
        curRow = ceiling(i/plotCols)
        curCol = (i-1) %% plotCols + 1
        print(plots[[i]], vp = vplayout(curRow, curCol ))
    }
}
```

## Nash-Sutcliffe Efficiency
The Nash-Sutcliffe Criterion $NSE$ is by far the most used efficiency criterion
in hydrology. In the hydrological context $x$ usually represents a set of
runoff-observation and $y$ a set of simulations. The $NSE$ is defined in the
same way as the general definition of the coefficient of determination $R^2$:

$$NSE = \sum_{t=1}^T \frac{x(t)-y(t)}{x(t)-\bar{x}}.$$

In this case the variable $\bar{x}$ represents the average of $x$. The $NSE$
can therefore be seen as the relational the estimator $y$ and the estimator
resulting form the average of the data. The following figure illustrates
different $NSE$ values for naive examples.
```{r, fig.align = "center", eval = TRUE, echo=FALSE, purl=FALSE}
# examples:
  plot_list <- list()
  for (i in 2:demo_length) {
    name_i <- demo_names[i]
    obj_i <- NSE( sim = demo_data[name_i],obs = demo_data["x"])
    title_i <- paste("NSE = ", round(obj_i,2), sep = "")
    plot_list[[i-1]] <- ggplot(demo_data) +
      geom_point(aes_string(x = "x", y = name_i)) +
      xlab( title_i ) +
      ylab("") +
      theme_minimal() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())
  }
 multiplot(plotlist = plot_list, cols = 3 )
```



## Kling-Gupta Efficiency
The Kling-Gupta Efficiency $KGE$ was introduced by Gupta et al. (2009) to
alleviate some of the shortcomings of the $NSE$. In their paper they argue
why the $NSE$ tends to overate simulations with small variance
(note: in the context of the paper $\textrm{simulations} = y$) and
propose the their efficiency criterion instead.

The $KGE$ is defined as:

$$ KGE = 1 - ED, $$

with

$$ ED = \sqrt{ (corr-1)^2 + (\alpha-1)^2 + (\beta-1)^2) }. $$

Where $\alpha = \frac{\sigma_y}{ \sigma_x }$ , i.e.: standard deviation $\sigma$ of the $y$ divided by
the $\sigma$  of $x$ and $\beta = \mu_y / \mu_x$ and $\mu$ is the arithmetic mean.
The following figure exemplifies its usage for the same data as above.
```{r, fig.align = "center", eval = TRUE, echo=FALSE, purl=FALSE}
# examples:
  plot_list <- list()
  for (i in 2:demo_length) {
    name_i <- demo_names[i]
    obj_i <- KGE( sim = demo_data[name_i],obs = demo_data["x"])
    title_i <- paste("KGE = ", round(obj_i,2), sep = "")
    plot_list[[i-1]] <- ggplot(demo_data) +
      geom_point(aes_string(x = "x", y = name_i)) +
      xlab( title_i ) +
      ylab("") +
      theme_minimal() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())
  }
 multiplot(plotlist = plot_list, cols = 3 )
```

## Percentage Bias
The percentage of bias $pBIAS$ is defined as the sum of the differences
between $x$ and $y$ divided by the sum of $x$:

$$pBIAS = 100*\frac{\sum_{} [ x(t)-y(t) ] }{\sum_{} x(t)}.$$

The $100*$ is just a scaling factor applied to express $pBIAS$ as a percentage.
The following figure illustrates different $pBIAS$ values for the same data
as above.
```{r, fig.align = "center", eval = TRUE, echo=FALSE, purl=FALSE}
# examples:
  plot_list <- list()
  for (i in 2:demo_length) {
    name_i <- demo_names[i]
    obj_i <- pbias( sim = demo_data[name_i],obs = demo_data["x"])
    title_i <- paste("pBIAS = ", round(obj_i,2), sep = "")
    plot_list[[i-1]] <- ggplot(demo_data) +
      geom_point(aes_string(x = "x", y = name_i)) +
      xlab( title_i ) +
      ylab("") +
      theme_minimal() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())
  }
  multiplot(plotlist = plot_list, cols = 3 )
```


## Pearson's correlation coefficient
The correlation coefficient $corr$ is a measure of the linear relationship
between $x$ and $y$. It is defined as:

$$corr = \frac{cov(x,y)}{\sigma^2_y*\sigma^2_x},$$

where $cov(...)$ denotes the covariance and $\sigma^2$ the variance. The correlation
coefficient can take on values between -1 and 1. The former corresponds to a
inverse and the latter to a direct relationship and the closer the values
is to zero the weaker the implied correlation. The following figure taken
from the illustrates some examples of $corr$ to give a feeling for its usage:

```{r, fig.align = "center", eval = TRUE, echo=FALSE, purl=FALSE}
# examples:
  plot_list <- list()
  for (i in 2:demo_length) {
    name_i <- demo_names[i]
    obj_i <- cor( x = demo_data[name_i], y = demo_data["x"])
    title_i <- paste("corr = ", round(obj_i,2), sep = "")
    plot_list[[i-1]] <- ggplot(demo_data) +
      geom_point(aes_string(x = "x", y = name_i)) +
      xlab( title_i ) +
      ylab("") +
      theme_minimal() +
      theme(axis.text.x = element_blank(),
            axis.text.y = element_blank())
  }
 multiplot(plotlist = plot_list, cols = 3 )
```


# Code
## `extract_objective_functions`
The purpose of this function is to extract a set of objective functions from
the `runoff_data` data.frame. The objective functions are extracted for each
basin, for the all the periods as a summary and for each period separately.

After the calculation of the `dplyr` is used to separate the different
periods (see also `mark_periods`) and get access to the pipe operator `%>%`.
As shown below the function checks if `runoff_data` is a data.frame and if the
periods are marked before the real computation starts.
```{r}
#' Get basic objective function for runoff_data
#'
#' Calculate basic objective functions
#'(NSE, KGE, percentage BIAS, Correlation (see: xxx)) for
#' every basin and the chosen periods
#'
#' @param runoff_data runoff_data data.frame (see:xxx).
#' @return list of basic objective function evaluated for the different
#' hydrological years and over the whole timespan.
#' @export
extract_objective_functions <- function(runoff_data) {
  require("hydroGOF", quietly = TRUE)
  require("dplyr", quietly = TRUE)
  assert_dataframe(runoff_data)
  stopifnot( exists(viscos_options()$name_COSperiod, where = runoff_data) )
```

The computational part of the function works as follows. In step (I) the
non-marked periods of `runoff_data` (i.e. columns of
`viscos_options()$name_COSperiod` which are smaller then > 0) are excluded from
further calculations. The thereby obtained data.frame is named
`evaluation_data`. Then, step (II) extracts important information for the
calculations:

- The number of basins (`number_of_basins`) is obtained by getting the names
of the data.frame, applying `tolower` to make them independent of upper and
lower cases and summing up all the name_data1 column,  
- the indices of the marked periods (`periods_in_data`) are obtained by getting
the unique values of the `viscos_options()$name_COSperiod` column, and
- number of periods (`number_of_periods`) are obtained by measuring the length
of `periods_in_data`

Step (III) computes the overall objective functions. This is done by copying all
`viscos_options()$name_data1` columns into the `temp_x` variable and all
`viscos_options()$name_data1` into the `temp_y` variable. The former is then
handed as observation and the latter as simulation to the `hydroGOF` functions.
The correlation `obj_fun$CORR` is calculated with base `R`. As `cor` returns
a whole cross-correlation matrix only the diagonal is extracted with `diag`.
Step (IV) is a period-wise a repetition of step (III). Thus,  at each each
iteration `k` the given period is filtered from `evaluation_data`
with `dpylr::filtr` before `temp_x` and `temp_y` is calculated.
The so obtained period-wise objective function values are marked by `_period`

```{r}
  # (I) reduce necessary computation
  evaluation_data <- runoff_data[
      runoff_data[[viscos_options()$name_COSperiod]] > 0,
    ]
  # (II) get information
  number_of_basins <- evaluation_data %>%
    names %>%
    unique %>%
    tolower %>%
    grepl(viscos_options()$name_data1 , .) %>%
    sum
  periods_in_data <- evaluation_data[[viscos_options()$name_COSperiod]] %>%
    unique
  number_of_periods <- periods_in_data %>% length
  # (III) calculate overall objective functions
  obj_fun  <- list()
  temp_x <- dplyr::select(evaluation_data,starts_with(viscos_options()$name_data1)) %>%
    unname
  temp_y <- dplyr::select(evaluation_data,starts_with(viscos_options()$name_data2)) %>%
    unname
  obj_fun$NSE <- hydroGOF::NSE(temp_y,temp_x)
  obj_fun$KGE <- hydroGOF::KGE(temp_y,temp_x)
  obj_fun$pBIAS <- hydroGOF::pbias(temp_y,temp_x)
  obj_fun$CORR <- cor(temp_y,temp_x) %>% diag(.)
  # (IV) calulcated period-vise objective functions
    # pre allocation of periodic variables:
    obj_fun$NSE_period <- matrix(nrow = number_of_periods, ncol = as.integer(number_of_basins), data = NA)
    obj_fun$KGE_period <- obj_fun$NSE_period
    obj_fun$pBIAS_periods <- obj_fun$NSE_period
    obj_fun$CORR_period <- obj_fun$NSE_period
    # calculation loop
    for (k in 1:number_of_periods)
    {
      temp_x <- dpylr::filter(evaluation_data,period == periods_in_data[k]) %>%
        dpylr::select(.,starts_with(viscos_options()$name_data1)) %>%
        unname
      temp_y <- dpylr::filter(evaluation_data,period == periods_in_data[k]) %>%
        dpylr::select(.,starts_with(viscos_options()$name_data2)) %>%
        unname
      obj_fun$NSE_period[k,1:number_of_basins] <- hydroGOF::NSE(temp_y,temp_x)
      obj_fun$KGE_period[k,1:number_of_basins] <- hydroGOF::KGE(temp_y,temp_x)
      obj_fun$pBIAS_period[k,1:number_of_basins] <- hydroGOF::pbias(temp_y,temp_x)
      obj_fun$CORR_period[k,1:number_of_basins] <- cor(temp_y,temp_x) %>% diag(.)
    }
  #
  return(obj_fun)
}
```

## `explore_basins_with_of`
This function is a [shiny App](http://shiny.rstudio.com/) which makes it
possible to explore the (hydro-) raphs for the different basins in conjunction 
with objective functions for the given time-window. The code is (still) a 
little bit crude, as severall variables are pushed to the global enviroment 
for it to work. This is suboptimal and will hopefully be improved upon in the 
future.

The app requires quite a lot of dependencies. Right now these are: 
 - `shiny`,to execute the whole app and make interactivity possible
```{r}
#' explore runoff_data with Objective Funcitons
#'
#' Runs a Shiny App which can be used to get an overview of a runoff_data time
#' series object. 
#'
#' @param d_xts runoff_data formatted as time series
#' @export
#' @examples
#' # get example data,
#' # clean it and
#' # explore the model performance
#' d_runoff <- get_runoff_example()
#' explore_runoff_with_ofun(d_runoff)
explore_runoff_with_ofun <- function(runoff_data) {
  require("shiny", quietly = TRUE)
  require("dplyr", quietly = TRUE)
  require("magrittr", quietly = TRUE)
  require("xts", quietly = TRUE)
  require("dygraphs", quietly = TRUE)
  ##########################
```

```{r}
  #$ this is all suboptimal! Maybe exploit the global function or something
  runoff_data %<>% remove_leading_zeros
  if ( !viscos_options( )$name_COSposix %in% names(runoff_data) ) {
    runoff_data %<>% prepare_complete_date()
  }
  runoff_data <<- runoff_data
  d_xts <<- runoff_as_xts(runoff_data)
  d_names_all<- names(d_xts)
  idx_names <- d_names_all %>% tolower %>% grepl("\\d" ,.)
  d_names <<- d_names_all[idx_names]
  d_nums <<- d_names %>% gsub("\\D","",.) %>% as.integer %>% unique

```

```{r}
  server <- function(input, output, session) {# executes calculation file
    # select the basin from the data
    #ยง Problem: QOBS%_02 assumes a formatted integer format ! This should not be, Maybe try "stringr"

    # get strings used in the naming of runoff_data
    unique_data_names <- names(runoff_data) %>% gsub("\\d","",.) %>% tolower %>% unique
    x_string <- unique_data_names[ unique_data_names %>% grep(viscos_options( )$name_data1,.) ]
    y_string <- unique_data_names[ unique_data_names  %>% grep(viscos_options( )$name_data2,.) ]
    #
    '%&%' <- function(a,b) paste(a,b,sep = "")
    selector_x <- reactive({ x_string %&% input$basin_num %&% "$" })
    selector_y <- reactive({ y_string %&% input$basin_num %&% "$" })
    selected_data <- reactive({
      select(runoff_data,
             matches( selector_x() ),
             matches( selector_y() )
      ) %>%
        select(Qobs = matches( selector_x() ),
               Qsim = matches( selector_y() ))
    })
    # create xts-formated table for use in dygraphs
    xts_selected_data <- reactive ({
      xts(selected_data(),order.by = runoff_data[[viscos_options()$name_COSposix]])
    })
    # plots
    output$dygrph1 <- renderDygraph({
      dygraph(xts_selected_data(), group="A") %>%
        dySeries("Qobs", 
                 label = visCOS::viscos_options()$name_data1,
                 color = viscos_options()$color_data1 ) %>%
        dySeries("Qsim", 
                 label = visCOS::viscos_options()$name_data2,
                 color = viscos_options()$color_data2 ) %>%
        dyOptions(includeZero = TRUE) %>%
        dyRangeSelector(height = 20, strokeColor = "")
    })
    # stats
    slctd_from <- reactive({
      if (!is.null(input$dygrph1_date_window))
        input$dygrph1_date_window[[1]]
    })
    slctd_to <- reactive({
      if (!is.null(input$dygrph1_date_window))
        input$dygrph1_date_window[[2]]
    })
    # stats header
    output$slctd_info <- renderText({
      if (!is.null(input$dygrph1_date_window))
        paste(strftime(slctd_from(), format = "%d %b %Y"),
              "-",
              strftime(slctd_to(), format = "%d %b %Y"),
              sep = " ")
    })
    # stats calc
    sub_slctd <- reactive({
      if (!is.null(input$dygrph1_date_window))
        xts_selected_data()[paste(strftime(slctd_from(), format = "%Y-%m-%d-%H-%M"),
                               strftime(slctd_to(), format = "%Y-%m-%d-%H-%M"),
                               sep = "/")]
    })

    output$slctd_OF <- renderTable({
      if (!is.null(input$dygrph1_date_window))
        out <- serve_ofun( sub_slctd()$Qobs,sub_slctd()$Qsim )
    })
  }
```

```{r}
  ui <- fluidPage(
      selectInput("basin_num",
                  "# basins:",
                  choices = d_nums,
                  selected = 1,
                  width = "100px"),
      dygraphOutput("dygrph1", width = "100%", height = "400px"),
      hr(),
      fluidRow(
        column(12, align = "center",
          textOutput("slctd_info"),
          tableOutput("slctd_OF")
        )
      )
    )
```

```{r}
  shinyApp(ui,server)
}
```


### `explore_ofun`
This function is a sub-function of `explore_basins_with_of` and basically simpler
version of `extract_objective_functions`. `explore_ofun` is not exported and
envisioned to be used within the shiny app.
It takes the two variables $x$ and $y$ (as used within this section)
and calculates the root mean squared error $RMSE = \sqrt{\sum(x(t)-y(t))^2}$
and the objective functions used within **visCOS** (see: introduction).
It also returns the $\alpha$ and $\beta$ of the $KGE$.
The former is the relation of the standard deviations and therefore a measure
of the of relative variability in the $y$ and $x$ values. The latter represents
the bias as a ratio of the two mean values of $x$ and $y$.

Including $\alpha$ and $\beta$ is supposed to enable users to get a bit more
intuition about the how the $KGE$ works. Personal experience suggests however
that the $KGE$ is difficult to grasp!
```{r}

# Get some objective functions (OF)
#
# Get some basic objective functions used in hydrology:
# Root Mean Squared Error, Correlation, NSE, KGE, pbias
# @return data.frame contianing basic OF
serve_ofun <- function(x,y) {
  require("hydroGOF", quietly = TRUE)
  require("magrittr", quietly = TRUE)
  # calc
  out <- data.frame(
    RMSE = rmse(y,x) %>% as.numeric,
    pbias = pbias(y,x) %>% as.numeric,
    NSE = NSE(y,x) %>% as.numeric,
    KGE = KGE(y,x) %>% as.numeric,
    corr = -cor(x,y) %>% diag(),
    beta =  mean(y)/mean(x),
    alpha =  sd(y)/sd(x)
  )
  return(out)
}

```



# References
- Gupta, Hoshin V.; Kling, Harald; Yilmaz, Koray K. & Martinez, Guillermo F.
(2009): *Decomposition of the mean squared error and NSE performance criteria:*
*Implications for improving hydrological modelling* (Journal of Hydrology)
- Anthony Damico at
http://stackoverflow.com/questions/7631799/what-does-correlation-coefficient-actually-represent
(retrieved at the 12.07.2016)
